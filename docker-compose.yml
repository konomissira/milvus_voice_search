version: "3.5"

services:
    # Milvus stack
    etcd:
        container_name: milvus-etcd
        image: quay.io/coreos/etcd:v3.5.18
        environment:
            - ETCD_AUTO_COMPACTION_MODE=revision
            - ETCD_AUTO_COMPACTION_RETENTION=1000
            - ETCD_QUOTA_BACKEND_BYTES=4294967296
            - ETCD_SNAPSHOT_COUNT=50000
        volumes:
            - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
        command: etcd -advertise-client-urls=http://etcd:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
        healthcheck:
            test: ["CMD", "etcdctl", "endpoint", "health"]
            interval: 30s
            timeout: 20s
            retries: 3

    minio:
        container_name: milvus-minio
        image: minio/minio:RELEASE.2024-12-18T13-15-44Z
        environment:
            MINIO_ACCESS_KEY: minioadmin
            MINIO_SECRET_KEY: minioadmin
        ports:
            - "9001:9001"
            - "9000:9000"
        volumes:
            - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
        command: minio server /minio_data --console-address ":9001"
        healthcheck:
            test:
                ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3

    standalone:
        container_name: milvus-standalone
        image: milvusdb/milvus:v2.6.0
        command: ["milvus", "run", "standalone"]
        security_opt:
            - seccomp:unconfined
        environment:
            ETCD_ENDPOINTS: etcd:2379
            MINIO_ADDRESS: minio:9000
            MQ_TYPE: woodpecker
        volumes:
            - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
            interval: 30s
            start_period: 90s
            timeout: 20s
            retries: 3
        ports:
            - "19530:19530" # Milvus gRPC
            - "9091:9091" # Milvus HTTP/metrics
        depends_on:
            - "etcd"
            - "minio"

    # App PostgreSQL (metadata DB)
    app-postgres:
        container_name: app-postgres
        image: postgres:15-alpine
        environment:
            POSTGRES_DB: ${APP_PG_DB}
            POSTGRES_USER: ${APP_PG_USER}
            POSTGRES_PASSWORD: ${APP_PG_PASSWORD}
        ports:
            - "5432:5432"
        volumes:
            - ./volumes/pgdata:/var/lib/postgresql/data
            - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres -d voice"]
            interval: 10s
            timeout: 5s
            retries: 5

    # Airflow
    airflow-redis:
        image: redis:7-alpine
        container_name: airflow-redis
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 10s
            timeout: 5s
            retries: 5

    airflow-db:
        image: postgres:13
        container_name: airflow-db
        environment:
            POSTGRES_USER: ${AIRFLOW_DB_USER}
            POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
            POSTGRES_DB: ${AIRFLOW_DB_NAME}
        volumes:
            - ./volumes/airflow-db:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
            interval: 10s
            timeout: 5s
            retries: 5

    airflow-init:
        build:
            context: ./docker/airflow
            dockerfile: Dockerfile
        image: local/airflow:2.9.3-ml
        container_name: airflow-init
        user: "0"
        entrypoint: /bin/bash
        environment:
            MILVUS_HOST: standalone
            MILVUS_PORT: "19530"

            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-db/${AIRFLOW_DB_NAME}
            AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
            AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
            AIRFLOW__CORE__LOAD_EXAMPLES: "False"
            _AIRFLOW_DB_UPGRADE: "true"
            _AIRFLOW_WWW_USER_CREATE: "true"
            AIRFLOW_ADMIN_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
            AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
            AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
            AIRFLOW_UID: "${AIRFLOW_UID:-50000}"

            # new: let Airflow import your src/* and find data/*
            PYTHONPATH: /opt/airflow
            DATA_DIR: /opt/airflow/data
        command:
            - -c
            - |
                set -e
                mkdir -p /opt/airflow/{logs,dags,plugins}
                chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins}
                airflow db init
                airflow users create \
                    --username "$AIRFLOW_ADMIN_USERNAME" \
                    --firstname Admin \
                    --lastname User \
                    --role Admin \
                    --email "$AIRFLOW_ADMIN_EMAIL" \
                    --password "$AIRFLOW_ADMIN_PASSWORD"
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
            - ./airflow/plugins:/opt/airflow/plugins
            # new mounts to map data and src folders
            - ./src:/opt/airflow/src
            - ./data:/opt/airflow/data
        depends_on:
            airflow-db:
                condition: service_healthy

    airflow-webserver:
        build:
            context: ./docker/airflow
            dockerfile: Dockerfile
        image: local/airflow:2.9.3-ml
        container_name: airflow-webserver
        command: webserver
        environment:
            MILVUS_HOST: standalone
            MILVUS_PORT: "19530"

            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-db/${AIRFLOW_DB_NAME}
            AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
            AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
            AIRFLOW_UID: "${AIRFLOW_UID:-50000}"

            # new: DAGs can import src.*, and ASR reads data/
            PYTHONPATH: /opt/airflow
            DATA_DIR: /opt/airflow/data

            #  expose APP_PG_* to tasks
            APP_PG_HOST: ${APP_PG_HOST:-app-postgres}
            APP_PG_DB: ${APP_PG_DB}
            APP_PG_USER: ${APP_PG_USER}
            APP_PG_PASSWORD: ${APP_PG_PASSWORD}
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
            - ./airflow/plugins:/opt/airflow/plugins
            # new mounts to map folders: src and data
            - ./src:/opt/airflow/src
            - ./data:/opt/airflow/data
        ports:
            - "8080:8080"
        healthcheck:
            test:
                ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
            interval: 20s
            timeout: 10s
            retries: 5
        depends_on:
            airflow-db:
                condition: service_healthy
            airflow-init:
                condition: service_completed_successfully
            app-postgres:
                condition: service_started
            standalone:
                condition: service_started

    airflow-scheduler:
        build:
            context: ./docker/airflow
            dockerfile: Dockerfile
        image: local/airflow:2.9.3-ml
        container_name: airflow-scheduler
        command: scheduler
        environment:
            MILVUS_HOST: standalone
            MILVUS_PORT: "19530"

            AIRFLOW__CORE__EXECUTOR: LocalExecutor
            AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-db/${AIRFLOW_DB_NAME}
            AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
            AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
            AIRFLOW_UID: "${AIRFLOW_UID:-50000}"

            PYTHONPATH: /opt/airflow
            DATA_DIR: /opt/airflow/data

            # pass app DB creds to tasks
            APP_PG_HOST: ${APP_PG_HOST:-app-postgres}
            APP_PG_DB: ${APP_PG_DB}
            APP_PG_USER: ${APP_PG_USER}
            APP_PG_PASSWORD: ${APP_PG_PASSWORD}
        volumes:
            - ./airflow/dags:/opt/airflow/dags
            - ./airflow/logs:/opt/airflow/logs
            - ./airflow/plugins:/opt/airflow/plugins
            # new mounts to map src and data folders
            - ./src:/opt/airflow/src
            - ./data:/opt/airflow/data
        depends_on:
            airflow-db:
                condition: service_healthy
            airflow-init:
                condition: service_completed_successfully
            app-postgres:
                condition: service_started
            standalone:
                condition: service_started

    api:
        build:
            context: ./api
            dockerfile: Dockerfile
        image: local/voice-search-api:latest
        container_name: voice-search-api
        environment:
            # Milvus in Docker
            MILVUS_HOST: standalone
            MILVUS_PORT: "19530"
            MILVUS_COLLECTION: call_embeddings

            # Postgres (app DB)
            APP_PG_HOST: ${APP_PG_HOST}
            APP_PG_DB: ${APP_PG_DB}
            APP_PG_USER: ${APP_PG_USER}
            APP_PG_PASSWORD: ${APP_PG_PASSWORD}

            # Optional: let API import to shared helpers
            PYTHONPATH: /opt/airflow:/opt/airflow/src
        volumes:
            # Mount your project root so /src is available
            - ./src:/opt/airflow/src:ro
            - ./data:/opt/airflow/data:ro # Mount data folder so API can serve audio files
        ports:
            - "8000:8000"
        depends_on:
            app-postgres:
                condition: service_started
            standalone:
                condition: service_started

    streamlit-ui:
        build:
            context: ./app
            dockerfile: Dockerfile
        image: local/voice-search-ui:latest
        container_name: voice-search-ui
        environment:
            API_BASE: http://api:8000 # for Streamlit server to call API
            PUBLIC_API_BASE: http://localhost:8000 # for the browser to fetch audio
        ports:
            - "8501:8501"
        depends_on:
            - api

networks:
    default:
        name: milvus
